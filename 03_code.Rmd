# Supplementary workflow {-}

## Curating databases for distance matrix  {-}

Required files : 

- barque COI [3](https://www.ibis.ulaval.ca/services/bioinformatique/barque_databases/) (`barque_coi.fasta`)
- RDP COI V5.1.0 [5](https://github.com/terrimporter/CO1Classifier/releases/tag/RDP-COI-v5.1.0) (`rdp_coi.fasta`)

### RDP {-}

1. Filter out sequence which do not belong to class Actinopteri

```{bash, eval = FALSE}
seqkit grep -r -n -p '.*Actinopteri*' rdp_coi.fasta -o chord_rdp_coi.fasta
```

2. Extract sequences header and print into txt file

```{bash, eval = FALSE}
grep ">" chord_rdp_coi.fasta | sed -e "s/>//" > rdp_coi_header.txt
```

3. The file with the sequence headers can then be passed to R to generate a list of which headers to keep. As per methodology by Edgar (2018), because the databases have a highly uneven numbers of sequences per genus, a subset is create by imposing a maximum of 10 sequences per genus. Headers are sampled at random to meet this constraint.

```{r, eval = FALSE}

#### ------------------------- Load libraries ------------------------------------------------------------------####  

library(dplyr)
library(tidyr)

#### ------------------------- Load databases sequence header -------------------------------------------------- #### 

rdpcoi = read.table("/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/rdp_coi_header.txt",sep = ";")

#### ------------------------- Format dataframes -------------------------------------------------------------- #### 
names(rdpcoi) = c("ID", "Cellular_organism","Superkingdom", "Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")
rdp_coi_actino = subset(rdpcoi, Class == "Actinopteri")

#### ------------------------- Random sampling  --------------------------------------------------------------- #### 

random_sampling_list = list()
i = 0 

for (genus in unique(rdp_coi_actino$Genus)){
  i = i + 1
  sub_df = subset(rdp_coi_actino, Genus == genus)
  if (nrow(sub_df) > 10) {
    rand_sub_df = sub_df[sample(nrow(sub_df), size=10), ] 
  } else {
    rand_sub_df = sub_df
  }
    random_sampling_list[[i]] = rand_sub_df
}

#### ------------------------- Generate list of headers to keep ----------------------------------------------- #### 

df = do.call("rbind", random_sampling_list)

df = df %>% unite("sequence_header", ID:Species, remove = FALSE, sep  = ";")
write.table(df$sequence_header, "/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/rdp_coi_headers_to_keep.txt", col.names = FALSE, row.names = FALSE, quote = FALSE)
```

4. Pass the file generated by the R script to seqkit to create a subset database 

```{bash, eval = FALSE}
seqkit grep -nrf rdp_coi_headers_to_keep.txt chord_rdp_coi.fasta -o curated_rdp_coi.fasta
``` 

### Barque {-}

1. Remove sequence of taxa which do not belong to phylum Chordata

```{bash, eval = FALSE}
seqkit grep -r -n -p '.*chordata_*' barque_coi.fasta -o chord_barque_coi.fasta
```

2. Add unique ID to every sequence header, this is required to randomly sample 10 sequences per genus

```{bash, eval = FALSE}
awk '/^>/{sub(">", ">"++i"_")}1' chord_barque_coi.fasta > chord_barque_coi_ID.fasta
```

3. Extract sequences header and print into txt file 

```{bash, eval = FALSE}
grep ">" chord_barque_coi_ID.fasta | sed -e "s/>//" > barque_coi_ID_header.txt
grep ">" rdp_coi.fasta | sed -e "s/>//" > rdp_coi_header.txt
```

4. The file with the sequence headers can then be passed to R to generate a list of which headers to keep. As per methodology by Edgar (2018), because the databases have a highly uneven numbers of sequences per genus, a subset is create by imposing a maximum of 10 sequences per genus. Headers are sampled at random to meet this constraint.

```{r, eval = FALSE}
#### ------------------------- Load libraries ------------------------------------------------------------------####  

library(dplyr)

#### ------------------------- Load databases sequence header -------------------------------------------------- #### 


barquecoi = read.table("/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/barque_coi_ID_header.txt", sep = "_")
rdpcoi = read.table("/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/rdp_coi_header.txt",sep = ";")

#### ------------------------- Format dataframes for merging --------------------------------------------------- #### 

# The sequence headers of the COI database used by barque has the following format : Phylum_Genus_Species
# Therefore, to keep only sequence belonging to class Actinopteri 
# I am merging the header from the barque COI database with headers from the RDP COI database to get complete 
# taxonomic rank

barquecoi$Species = paste(barquecoi$V2, barquecoi$V3, sep = "_") # Create column with specie name 
barquecoi_unique = barquecoi[!duplicated(barquecoi), ] # Keeping only unique value

# Add column names to RDP database 
names(rdpcoi) = c("ID", "Cellular_organism","Superkingdom", "Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")
rdpcoi_noID = subset(rdpcoi, select = -c(ID)) # Remove column ID (they are unique values)
rdpcoi_unique = rdpcoi_noID[!duplicated(rdpcoi_noID), ] # Keeping only unique value

#### ------------------------- Merge dataframes  -------------------------------------------------------------- #### 

comb_df = merge(barquecoi_unique, rdpcoi_unique, by = "Species", all.y = FALSE )

#### ------------------------- Random sampling  --------------------------------------------------------------- #### 

random_sampling_list = list()
i = 0 

for (genus in unique(comb_df$Genus.x)){
  i = i + 1
  sub_df = subset(comb_df, Genus.x == genus)
  if (nrow(sub_df) > 10) {
    rand_sub_df = sub_df[sample(nrow(sub_df), size=10), ] 
  } else {
    rand_sub_df = sub_df
  }
    random_sampling_list[[i]] = rand_sub_df
}

df = do.call("rbind", random_sampling_list)

#### ------------------------- Generate list of headers to keep ----------------------------------------------- #### 

df$barque_coi_header = paste(df$ID, df$Phylum.x, df$Species, sep = "_")

write.table(df$barque_coi_header, "/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/barque_coi_headers_to_keep.txt",col.names = FALSE, row.names = FALSE, quote = FALSE)
```

5. Pass the file generated by the R script to seqkit to create a subset database. 

```{bash, eval = FALSE}
seqkit grep -nrf barque_coi_headers_to_keep.txt chord_barque_coi.fasta -o curated_barque_coi.fasta
``` 

## Worklow used {-}

### DADA2 {-}

#### 12S {-}

#### COI {-}

839 ASV | from these 839 ASVs, 20 (2.38 %) are species with a bootstrap level above 70 % | from these 20 species, 8 (40 %) are from class actinopteri | from these 8 actinopteri, 7 (87.5 %) are species of freshwater fishes found in Alberta. 

### Barque {-}

NB. Barque will only work on GNU Linux or OSX

1. Install dependencies

- bash 4+
- python 3.5+ (you can use miniconda3 to install python)
- python distutils package
- R 3+ (ubuntu/mint: sudo apt-get install r-base-core)
- java (ubuntu/mint: sudo apt-get install default-jre)
- gnu parallel
    - For mac OSX `brew install parallel` 
- flash (read merger) v1.2.11+
   - `export PATH=/home/user/FLASH-1.2.11:$PATH`
- vsearch v2.14.2+ (Barque will not work with older versions of vsearch)
   - Depending on how vsearch is installed either export PATH to location of vsearch or activate base conda environnement

2. Download a copy of the Barque repository

```{bash, eval = FALSE}
git clone https://github.com/enormandeau/barque
```

3. Edit 02_info/primers.csv to provide information describing your primers and database to use 

4. Get or prepare the database(s) (see Formatting database section below) and deposit the fasta.gz file in the 03_databases folder and give it a name that matches the information of the 02_info/primers.csv file.

 - [Link](https://www.ibis.ulaval.ca/services/bioinformatique/barque_databases/) to BOLD database 

5. Modify the parameters in 02_info/barque_config.sh for your run

- for COI marker : 

`COI_kv,CGTATTTGGYGCYTGRGCCGGRATAGT,CARAARCTYATRTTRTTYATTCG,100,450,bold,0.97,0.9,0.85`

- for 12S marker 

`12s200pb,GTCGGTAAAACTCGTGCCAGC,CATAGTGGGGTATCTAATCCCAGTTTG,150,350,12S,0.98,0.9,0.85`

6. Launch Barque 

Move into the Github directory and launch barque 
```{bash, eval = FALSE}
./barque 02_info/barque_config.sh
```

Once the pipeline has finished running, all result files are found in the 12_results folder.
After a run, it is recommended to make a copy of this folder with some additional info

```{bash, eval = FALSE}
cp -r 12_results 12_results_PROJECT_NAME_DATE_SOME_ADDITIONAL_INFO
``` 

