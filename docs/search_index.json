[["index.html", "Our dataset Alberta fishes Worklfows and their associated available databases Comparing pipelines with available databases Curating databases", " Our dataset 18 water samples collected from rivers and lakes near oil-sands extraction sites in the province of Alberta, Canada. Sampled water was filtered and DNA collected on the filers was extracted and sequenced. For each sample collected, two different marker genes were used to amplify and sequence distinct DNA regions : the 12S ribosomal RNA gene and the mitochondrial cytochrome oxidase subunit (COI). Alberta fishes Fishbase was used to generate the following list of Alberta freshwater fishes. Worklfows and their associated available databases Links [1] Barcode of life (BOLD) [2] barque workflow [3] COI formatted for Barque [4] DADA2 workflow [5] Eukaryote CO1 Classifier [6] Mitochondrial Genome Database of Fish (MitoFish) [7] National Center for Biotechnology Information (NCBI) [8] Ribosomal Database Project (RDP) classifier [9] VSEARCH [10] 12S fish Classifier v1.0.1 [11] 12S formatted for barque Representation of Alberta freshwater fishes in databases Each databases were queried to evaluate the presence of reference sequences for Alberta freshwater fishes. Code used can be found here. Comparing pipelines with available databases DADA2 12S COI 839 ASV | from these 839 ASVs, 20 (2.38 %) are species with a bootstrap level above 70 % | from these 20 species, 8 (40 %) are from class actinopteri | from these 8 actinopteri, 7 (87.5 %) are species of freshwater fishes found in Alberta. Barque Curating databases https://forum.qiime2.org/t/building-a-coi-database-from-bold-references/16129 Using seqkit tool to filter databases in order to keep only sequences of freshwater fishes found in Alberta seqkit grep -v -n -f id_list.txt in.fasta &gt; out.fasta "],["taxonomy-cross-validation-by-identity-taxxi.html", "Taxonomy cross validation by identity (TAXXI) Generate benchmark datasets Generate predictions for RDP Classifier Generate predictions for VSEARCH CVI metrics", " Taxonomy cross validation by identity (TAXXI) Framework developed by Edgard (2018). Link to article and TAXXI website. Cross-validation by identity (CVI) models varying distances between query sequences and reference sequences. A reference with known taxonomies is split into test and training sets such that for all test sequences, the most similar training sequence has a given identity (d). This is repeated for different identities, enabling assessment of prediction accuracy at varying distances from the reference. Important definitions : The lowest common rank (LCR) of two sequences is the lowest rank where both have the same taxon name. The most probable lowest common rank (MLR) for a pair of sequences with identity d is defined as the LCR with highest probability. MLRs can be summarized by giving the rank identity threshold (RIT) for each rank r. The rank identity threshold (RIT) for each rank r is defined as the minimum identity for which MLR(d) = r. For example, if MLR(100) = species, MLR(99) = genus, MLR(98) = genus, … MLR(94) = genus and MLR(93) = family, then RIT(species) = 100 and RIT(genus) = 94. The top-hit identity distribution (THID) is the distances from a reference database. - R is the reference dataset divided into four disjoint subsets S, T, W and Z. - S is the test set. - A is the training set formed by the union of T and W. - T is the set of top hits for sequences in S, which are constrained to have identities in the range d ± σ (where σ specifies the maximum allowed deviation from the desired identity (d)). - W contains reference sequences with identity &lt; d; these are retained to create the largest possible training set. - Z contains sequences which cannot be assigned to S, T or W without violating the identity constraint. CVI performance metrics : Generate benchmark datasets The distmx_split_identity command from USEARCH divides sequences into subsets such that the top-hit identity is a given value. This is used to create test-training pairs for cross-validation by identity. Input is a distance matrix created by the calc_distmx command. As per the methods specified in Edgar (2018) maximum allowed deviation (σ) from d used : σ = 1% for d = 90% and σ = 0.5% for d = 99, 97 and 95%. The following code was adapted from Donhauser et al. (2024) and validated with available documentation from USEARCH. Testing with available databases (barque COI 3, RDP COI V5.1.0 5, RDP 12S 10 and barque 12S 11)) Remove tab spaces in sequence name (for RDP databases specifically) for i in *.fasta ; do sed -i &#39;s/\\t/;/g&#39; $i ; done To create the distance matrix the following code was executed from a bash script called run_distmx.sh using nohup. for i in *.fasta ; do ~/usearch -calc_distmx $i -maxdist 0.2 -termdist 0.3 -tabbedout ${i%%.*}_distmax.txt; done !!! Issue with barque_coi : Memory limit of 32-bit process exceeded, 64-bit build required.. To create training and test datasets at different % identity the following code was executed from a bash script called run_splitid.sh using nohup. for i in *.fasta ; do ~/usearch -distmx_split_identity ${i%%.*}_distmax.txt -mindist 0.025 -maxdist 0.035 -tabbedout ${i%%.*}.97.subsets.txt ~/usearch -distmx_split_identity ${i%%.*}_distmax.txt -mindist 0.045 -maxdist 0.055 -tabbedout ${i%%.*}.95.subsets.txt ~/usearch -distmx_split_identity ${i%%.*}_distmax.txt -mindist 0.095 -maxdist 0.105 -tabbedout ${i%%.*}.90.subsets.txt done Output is a tabbed text given by the -tabbedout option. Fields are: Col1 - Subset name : there are four subsets with names 1, 2, 1x and 2x. Col2 - Label1 : label1 is the label of a sequence in the subset given by Col1. Col3 - Label2 : label2 is the top hit in the other subset (1 or 2) Col4 - Dist : distance between Label1 and Label2. Subsets 1 and 2 have top hits to each other in the specified range. Subset 1x has lower identities with subset 2, and can therefore be added to the training set if subset 2 is the query set. Similarly, subset 2x has lower identities with subset 1 and can be added to the training set if subset 1 is the query Get sequence ID for training and test set. for i in *.subsets.txt ; do awk &#39;$1==1 || $1==&quot;1x&quot; {print $2}&#39; $i &gt; $i.trainingIDs.txt; awk &#39;$1==2 {print $2}&#39; $i &gt; $i.testIDs.txt; done Create fasta file with test and training set at each identity, use subset 1 as training and subset 2 as test. If sekqit is installed throught conda activate base environnement first. for i in *.trainingIDs.txt ; do seqkit grep -n -f $i ${i%%.*}.fasta &gt; $i.trainning.fasta ; done for i in *.testIDs.txt ; do seqkit grep -n -f $i ${i%%.*}.fasta &gt; $i.test.fasta ; done Renaming the files to generate shorter names in following code for file in *.trainning.fasta ; do mv $file ${file//_rdp./_rdp_} ; done for file in *.test.fasta ; do mv $file ${file//_rdp./_rdp_} ; done Generate predictions for RDP Classifier Notes : To install the RDP classifier I recommend downloading the package from source forge (link) as the Github is no longer maintained. Once the download is complete, unzip the folder and locate the file classifer.jar inside the directory dist. Following steps were adapted from John Quensen’s tutorial (link) and available scripts associated with publication by Miranda (2020). Required files To train the newly generated training set for each identity level you need to generate a sequence file and a taxonomy file, each with special formatting requirements : Sequence file The sequence file must be in fasta format and contain a unique identifier without any white space. The accession number makes a good identifier. Anything after the first white space is ignored. The following are acceptable: &gt;DQ248313 ACGATTTTGACCCTTCGGGGTCGATCTCCAACCCTTCGGGGTCGATCGATTTTGACCCT &gt;JF735302 k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Hypocreales;f__Nectriaceae;g__Dactylonectria;s__Dactylonectria_anthuriicol CCGAGTTTTCAACTCGACCCTTCGGGGTCGTCGATCTCCAACCCGATCGATTTTGAACC Taxonomy file The taxonomy file is a tab-delimited text file beginning with a header giving the Sequence ID and names of ranks to be included. There are two requirements for this file: There must be an entry for every rank in every line. Hyphen placeholders are allowed but are not recommended. “Convergent evolution” is not allowed. For example, the same genus cannot appear in two different families. Options for missing ranks : If a rank does not exist, you can fill in the missing entries with hyphens but it is not recommended as it can lead to a “ragged” classification that cannot be properly sorted by rank. Another option is to fill in the empty spaces with made-up but meaningful ranks as in the table below. The prefixes indicate the highest rank available. The absence of hyphen placeholders means that classification will not be ragged but include all ranks. Thus it will be possible to select, sort, and merge ranks when analyzing your data later. Example format : Seq_ID Kingdom Phylum Class Order Family Genus Species MG190602 Fungi Ascomycota Sordariomycetes Hypocreales o_Hypocreales o_f_Hypocreales MF120484 Fungi Ascomycota Sordariomycetes Hypocreales Nectriaceae Fusarium Scripts lineage2taxTrain.py addFullLineage.py Generate required files Taxonomy file Extract sequence header using grep. for i in *.trainning.fasta ; do grep -e &quot;&gt;&quot; $i &gt; ${i%%.*}_trainheader.txt ; done Replace semi-colon with tabs and remove “&gt;”. for i in *.trainingIDs.txt ; do sed &#39;s/;/\\t/g&#39; $i &gt; ${i%%.*}.RDP_trainID.txt ; done for i in *.RDP_trainID.txt ; do sed -i &#39;s/&gt;//g&#39; $i ; done Add header (rank followed by column number i.e. rank_1, rank_2, etc.). for i in *.RDP_trainID.txt ; do awk -i inplace &#39;BEGIN {OFS=FS=&quot;\\t&quot;} NR==1{for (i=1;i&lt;=NF;i++) printf &quot;%s%s&quot;, &quot;rank_&quot;i, i==NF?ORS:OFS}1&#39; $i ; done Details : BEGIN {OFS=FS=“} sets the input and output delimiter to tab, instead of the default space. Change” to your delimiter if its not tab. NR==1{} says to execute the actions only on the first line TO DO : Pipe these commands together Sequence file Replace semi-colon with tabs for i in *trainning.fasta ; do sed -i &#39;s/;/\\t/g&#39; $i ; done for i in *.RDP_trainID.txt ; do python2 lineage2taxTrain.py $i &gt; ${i%%.*}.ready4train_tax.txt ; done for i in *.trainning.fasta ; do python2 addFullLineage.py ${i%%.*}.RDP_trainID.txt $i &gt; ${i%%.*}.ready4train_seqs.fasta ; done Train the set for i in *.RDP_trainID.txt ; do java -Xmx10g -jar ~/rdp_classifier_2.14/dist/classifier.jar train -o ${i%%.*}_training_files -s ${i%%.*}.ready4train_seqs.fasta -t $i ; done Output is a directory specified by the parameter -o which should contain the following files : bergeyTrainingTree.xml genus_wordConditionalProbList.txt logWordPrior.txt wordConditionalProbIndexArr.txt Move into this newly created directory and create the file rRNAClassifier.properties with the following text : # Sample ResourceBundle properties file bergeyTree=bergeyTrainingTree.xml probabilityList=genus_wordConditionalProbList.txt probabilityIndex=wordConditionalProbIndexArr.txt wordPrior=logWordPrior.txt classifierVersion=RDP Naive Bayesian rRNA Classifier Version 2.14 Generate predictions for i in *.ready4train_seqs.fasta ; do java -Xmx1g -jar ~/rdp_classifier_2.14/dist/classifier.jar -q ${i%%.*}.subsets.txt.testIDs.txt.test.fasta -t ./${i%%.*}_training_files/rRNAClassifier.properties -o ${i%%.*}.predictions.tsv ; done Generate predictions for VSEARCH CVI metrics Important definitions : N : number of sequences in the test set S, K : number of sequences in S with known names (names which are present in the training set A) L : number of novel test sequences (= N – K) (sequences in S with names that are not present in A) TP : number of names which are correctly predicted MC : number of misclassification errors OC : number of over-classification errors UC : number under-classification errors The rate for each type of error is defined as the number of errors divided by the number of opportunities to make that error: OCR = OC/L (over-classification rate), UCR = UC/K (under-classification rate) MCR = MC/K (misclassification rate) TPR = TP/K Acc = TP/(K + OC) For each rank the mean values of the metrics over all test/training pairs for all values of the top-hit identity (d) was calculated and is designated by prefix Avg. True-positive rate (AvgTPR) Under-classification errors (AvgUCR) Misclassification rate (AvgMCR) Over-classification rate (AvgOCR) Average L10Acc Average accuracy (AvgAcc) "],["code.html", "Code Evaluating representation in databases Workflows Curating databases", " Code Evaluating representation in databases #### ------------------------- Load libraries ------------------------------------------------------#### library(Biostrings) library(tidyverse) #### ------------------------- Load list of Freshwater fishes from Alberta ------------------------- #### alberta_fish = read.csv(&quot;/home/kvilleneuve/fish_edna/database/fishbase_alberta_freswater.csv&quot;, header = TRUE, check.names = FALSE) alberta_fish$Species = gsub(&quot; &quot;, &quot;_&quot;, alberta_fish$Species) # Replace space to underscore ### ------------------------- Load the databases -------------------------------------------------- #### ## RDP classifier ## rdpcoi_raw = readDNAStringSet(&quot;/home/kvilleneuve/fish_edna/database/rdp_coiv5_1_0/mydata_ref/mytrainseq.fasta&quot;, format = &quot;fasta&quot;) rdp12s_raw = readDNAStringSet(&quot;/home/kvilleneuve/fish_edna/database/12Sfishclassifier/mydata_training/mytrainseq.fasta&quot;, format = &quot;fasta&quot;) ## barque ## barquecoi_raw = readDNAStringSet(&quot;/home/kvilleneuve/fish_edna/database/bold_coi_for_barque_2023-09-12.fasta&quot;, format = &quot;fasta&quot;) barque12s_raw = readDNAStringSet(&quot;/home/kvilleneuve/fish_edna/database/barque_12S.fasta&quot;, format = &quot;fasta&quot;) ### ------------------------- Parse databases dataframes ----------------------------------------- #### # Because the number of ranks differ between databases used with # the RDP classifier and the barque workflow they are processed separately ## RDP databases list_rdp_db = list(&quot;COI RDP&quot; = rdpcoi_raw, &quot;12S RDP&quot; = rdp12s_raw) list_rdp_taxa = list() i = 0 for (databases in list_rdp_db){ i = i + 1 rdp_db = as.data.frame(databases@ranges@NAMES) rdp_taxa = rdp_db %&gt;% separate(`databases@ranges@NAMES`, c(&quot;Domain&quot;, &quot;Superkingdom&quot;, &quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;, &quot;Order&quot;, &quot;Family&quot;, &quot;Genus&quot;, &quot;Species&quot;), &quot;;&quot;) list_rdp_taxa[[i]] = rdp_taxa names(list_rdp_taxa)[i] = names(list_rdp_db[i]) } ## barque databases list_barque_db = list(&quot;12S barque&quot; = barque12s_raw, &quot;COI barque&quot; = barquecoi_raw) list_barque_taxa = list() i = 0 for (databases in list_barque_db){ i = i + 1 barque_db = as.data.frame(databases@ranges@NAMES) barque_taxa = barque_db %&gt;% separate(`databases@ranges@NAMES`, c(&quot;Family&quot;, &quot;Genus&quot;, &quot;sort_species&quot;), &quot;_&quot;) barque_taxa$Species = paste(barque_taxa$Genus,&quot;_&quot;,barque_taxa$sort_species, sep =&quot;&quot;) list_barque_taxa[[i]] = barque_taxa names(list_barque_taxa)[i] = names(list_barque_db[i]) } # After parsing each databases we combine them into as single list # then we filter out fishes which are not found in the list of Alberta freshwater fishes jointlist_all_db = c(list_rdp_taxa, list_barque_taxa) list_filtered_db = list() i = 0 for (databses in jointlist_all_db){ i = i + 1 filtered_db = databses %&gt;% filter(Species %in% unique(alberta_fish$Species)) filtered_db_df = as.data.frame(unique(filtered_db$Species)) names(filtered_db_df) = &quot;Species&quot; filtered_db_df[names(jointlist_all_db[i])] = &quot;Yes&quot; list_filtered_db[[i]] = filtered_db_df } filtered_df = Reduce(function(...) merge(..., all=T), list_filtered_db) ### ------------------------- Combine with dataframe of Alberta freshwater fishes and save output as csv ----------------------------------------- #### final_df = merge(alberta_fish, filtered_df, by = &quot;Species&quot;, all = TRUE) final_df = gsub(&quot;_&quot;, &quot; &quot;, final_df$Species) write.csv(final_df, &quot;/home/kvilleneuve/fish_edna/results/represensation_albertafish_databases.csv&quot;, quote = FALSE) Workflows DADA2 12S COI 839 ASV | from these 839 ASVs, 20 (2.38 %) are species with a bootstrap level above 70 % | from these 20 species, 8 (40 %) are from class actinopteri | from these 8 actinopteri, 7 (87.5 %) are species of freshwater fishes found in Alberta. Barque Curating databases Using seqkit tool to filter databases in order to keep only sequences of freshwater fishes found in Alberta seqkit grep -v -n -f id_list.txt in.fasta &gt; out.fasta https://forum.qiime2.org/t/building-a-coi-database-from-bold-references/16129 "],["supplementary-data.html", "Supplementary data Installing the RDP classifier", " Supplementary data Installing the RDP classifier The RDP Classifier is a naive Bayesian classifier that provides rapid and accurate taxonomic placement of rRNA gene sequences. Download the RDPTools is a collection fo command-line tools written by the RDP staff at Michigan State University. More information and tutorials on how to install, use and retrain RDP Clasifier can be found on at https://github.com/rdpstaff/classifier and John Quensen’s blog (https://john-quensen.com/). Unfortunately, the The RDP website is no longer available and therefore the packages must be installed using the Docker verison of RDPTools. instruction where obtained from Dr. J.Quensen’s Website. Installing Docker Engine The following installation instruction were obtained directly from the Docker documentation page # --------------------- Run the following command to uninstall all conflicting packages ------------------------ for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done # --------------------- Add Docker&#39;s official GPG key ------------------------ sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # ------------------------ Add the repository to Apt sources ------------------------ echo \\ &quot;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release &amp;&amp; echo &quot;$VERSION_CODENAME&quot;) stable&quot; | \\ sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null sudo apt-get update # ------------------------ Install the Docker packages ------------------------ sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin # ------------------------ Confirm if installation was successful ------------------------ sudo docker run hello-world Creating the container To display all locally available docker images : sudo docker images sudo docker pull rdpstaff/rdp_tools Containers must be map to a directory outside of the container in order to make results accessible to other programs. In this case I chose the directory /home/kvilleneuve/rdp_tools/ sudo docker run --name rdp_tools -it -v /home/kvilleneuve/rdp_tools/ rdpstaff/rdp_tools After entering the above command the prompt terminal should be similar to this : RDPuser@1041715dcc7f:~$ Installing auxiliary programs and files Change to root user. Password (RDPuser) sudo su - # Download script for downloading packages wget https://github.com/jfq3/RDPTools-Docker/raw/main/downloads/download_tools_jq.py # NOTE !!! The script contains a few typo, notable a missing parenthesis and open tick at line 58 !!!! # Make script executable chmod 750 download_tools_jq.py # Execute python3 download_tools_jq.py Log out, exit the container and close the terminal : exit exit exit To launch docker sudo docker start rdp_tools sudo docker attach rdp_tools "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
