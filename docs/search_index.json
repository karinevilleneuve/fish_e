[["index.html", "Our dataset Alberta fishes Methods and their associated available databases Representation of Alberta freshwater fishes in databases", " Our dataset The dataset consists of 18 water samples obtained from rivers and lakes near oil-sands extraction sites in the province of Alberta, Canada. Water samples were filtered and DNA collected on the filters was extracted and sequenced. For each sample, two different marker genes were used to amplify and sequence distinct DNA regions : the 12S ribosomal RNA gene and the mitochondrial cytochrome oxidase subunit (COI). Alberta fishes Fishbase was used to generate the following list of Alberta freshwater fishes. Methods and their associated available databases Within the framework of this project, the following methods and their associated databases were evaluated in regards to their capacities of correctly identifying sequences belonging to freshwater fishes from Alberta. Links [1] Barcode of life (BOLD) [2] barque workflow [3] COI formatted for Barque [4] DADA2 workflow [5] Eukaryote CO1 Classifier [6] Mitochondrial Genome Database of Fish (MitoFish) [7] National Center for Biotechnology Information (NCBI) [8] Ribosomal Database Project (RDP) classifier [9] VSEARCH [10] 12S fish Classifier v1.0.1 [11] 12S formatted for barque Representation of Alberta freshwater fishes in databases Each databases were queried to evaluate the presence of reference sequences for Alberta freshwater fishes. Code used to queried the databases can be found here. The absence of the following fishes from any databases can be attributed to different names being used. "],["taxonomy-cross-validation-by-identity-taxxi.html", "Taxonomy cross validation by identity (TAXXI) Generate benchmark datasets Generate predictions for RDP Classifier Generate predictions for VSEARCH CVI metrics", " Taxonomy cross validation by identity (TAXXI) Framework developed by Edgard (2018). Link to article and TAXXI website. Cross-validation by identity (CVI) models varying distances between query sequences and reference sequences. A reference with known taxonomies is split into test and training sets such that for all test sequences, the most similar training sequence has a given identity (d). This is repeated for different identities, enabling assessment of prediction accuracy at varying distances from the reference. - R is the reference dataset divided into four disjoint subsets S, T, W and Z. - S is the test set. - A is the training set formed by the union of T and W. - T is the set of top hits for sequences in S, which are constrained to have identities in the range d ± σ (where σ specifies the maximum allowed deviation from the desired identity (d)). - W contains reference sequences with identity &lt; d; these are retained to create the largest possible training set. - Z contains sequences which cannot be assigned to S, T or W without violating the identity constraint. Generate benchmark datasets The distmx_split_identity command from USEARCH divides sequences into subsets such that the top-hit identity is a given value. This is used to create test-training pairs for cross-validation by identity. Input is a distance matrix created by the calc_distmx command. As per the methods specified in Edgar (2018) maximum allowed deviation (σ) from d used : σ = 1% for d = 90% and σ = 0.5% for d = 99, 97 and 95%. The following code was adapted from Donhauser et al. (2024) and validated with available documentation from USEARCH. Testing with available databases (barque COI 3, RDP COI V5.1.0 5, RDP 12S 10 and barque 12S 11)) Remove tab spaces in sequence name (for RDP databases specifically). for i in *.fasta ; do sed -i &#39;s/\\t/;/g&#39; $i ; done Before : &gt;NC_003178 cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Ateleopodiformes;Ateleopodidae;Ateleopus;Ateleopus_japonicus After : &gt;NC_003178;cellularOrganisms;Eukaryota;Metazoa;Chordata;Actinopteri;Ateleopodiformes;Ateleopodidae;Ateleopus;Ateleopus_japonicus All the sequences from the COI used with RDP were lowercase. The following code was used to make them uppercase : seqkit seq mytrainseq.fasta --upper-case -w 0 &gt; rdp_coi.fasta Create a distance matrix using function -calc_distmx for i in *.fasta ; do ~/usearch -calc_distmx $i -maxdist 0.2 -termdist 0.3 -tabbedout ${i%%.*}_distmax.txt; done Considering the size of the original COI databases, the memory limit of of 32-bit process was exceeded, and therefore the 64-bit build was required. To overcome this issue I filtered out non-fish sequence from the database. The code used can be found here. To create training and test datasets at different identity thresholds the following code was executed from a bash script called run_splitid.sh using nohup. for i in *.fasta ; do ~/usearch -distmx_split_identity ${i%%.*}_distmax.txt -mindist 0.025 -maxdist 0.035 -tabbedout ${i%%.*}.97.subsets.txt ~/usearch -distmx_split_identity ${i%%.*}_distmax.txt -mindist 0.045 -maxdist 0.055 -tabbedout ${i%%.*}.95.subsets.txt ~/usearch -distmx_split_identity ${i%%.*}_distmax.txt -mindist 0.095 -maxdist 0.105 -tabbedout ${i%%.*}.90.subsets.txt done Output is a tabbed text given by the -tabbedout option. Fields are: Col1 - Subset name : there are four subsets with names 1, 2, 1x and 2x. Col2 - Label1 : label1 is the label of a sequence in the subset given by Col1. Col3 - Label2 : label2 is the top hit in the other subset (1 or 2) Col4 - Dist : distance between Label1 and Label2. Subsets 1 and 2 have top hits to each other in the specified range. Subset 1x has lower identities with subset 2, and can therefore be added to the training set if subset 2 is the query set. Similarly, subset 2x has lower identities with subset 1 and can be added to the training set if subset 1 is the query Get sequence ID for training and test set. for i in *.subsets.txt ; do awk &#39;$1==1 || $1==&quot;1x&quot; {print $2}&#39; $i &gt; $i.trainingIDs.txt; awk &#39;$1==2 {print $2}&#39; $i &gt; $i.testIDs.txt; done Create fasta file with test and training set at each identity, use subset 1 as training and subset 2 as test. If sekqit is installed throught conda activate base environnement first. for i in *.trainingIDs.txt ; do seqkit grep -n -f $i ${i%%.*}.fasta &gt; $i.trainning.fasta ; done for i in *.testIDs.txt ; do seqkit grep -n -f $i ${i%%.*}.fasta &gt; $i.test.fasta ; done Rename files for file in *subsets* ; do mv $file ${file//.9/_9} ; done Generate predictions for RDP Classifier Following steps were adapted from John Quensen’s tutorial (link) and available scripts associated with publication by Miranda (2020). Required files To train the newly generated training set for each identity level you need to generate a sequence file and a taxonomy file, each with special formatting requirements : Sequence file The sequence file must be in fasta format and contain a unique identifier without any white space. The accession number makes a good identifier. Anything after the first white space is ignored. The following are acceptable: &gt;DQ248313 ACGATTTTGACCCTTCGGGGTCGATCTCCAACCCTTCGGGGTCGATCGATTTTGACCCT &gt;JF735302 k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Hypocreales;f__Nectriaceae;g__Dactylonectria;s__Dactylonectria_anthuriicol CCGAGTTTTCAACTCGACCCTTCGGGGTCGTCGATCTCCAACCCGATCGATTTTGAACC Taxonomy file The taxonomy file is a tab-delimited text file beginning with a header giving the Sequence ID and names of ranks to be included. There are two requirements for this file: There must be an entry for every rank in every line. Hyphen placeholders are allowed but are not recommended. “Convergent evolution” is not allowed. For example, the same genus cannot appear in two different families. Options for missing ranks : If a rank does not exist, you can fill in the missing entries with hyphens but it is not recommended as it can lead to a “ragged” classification that cannot be properly sorted by rank. Another option is to fill in the empty spaces with made-up but meaningful ranks as in the table below. The prefixes indicate the highest rank available. The absence of hyphen placeholders means that classification will not be ragged but include all ranks. Thus it will be possible to select, sort, and merge ranks when analyzing your data later. Example format : Seq_ID Kingdom Phylum Class Order Family Genus Species MG190602 Fungi Ascomycota Sordariomycetes Hypocreales o_Hypocreales o_f_Hypocreales MF120484 Fungi Ascomycota Sordariomycetes Hypocreales Nectriaceae Fusarium Scripts lineage2taxTrain.py addFullLineage.py Generate required files Taxonomy file Extract sequence header using grep. for i in *.trainning.fasta ; do grep -e &quot;&gt;&quot; $i &gt; ${i%%.*}_trainheader.txt ; done Replace semi-colon with tabs and remove “&gt;”. for i in *.trainingIDs.txt ; do sed &#39;s/;/\\t/g&#39; $i &gt; ${i%%.*}.RDP_trainID.txt ; done for i in *.RDP_trainID.txt ; do sed -i &#39;s/&gt;//g&#39; $i ; done Add header (rank followed by column number i.e. rank_1, rank_2, etc.). for i in *.RDP_trainID.txt ; do awk -i inplace &#39;BEGIN {OFS=FS=&quot;\\t&quot;} NR==1{for (i=1;i&lt;=NF;i++) printf &quot;%s%s&quot;, &quot;rank_&quot;i, i==NF?ORS:OFS}1&#39; $i ; done Details : BEGIN {OFS=FS=“} sets the input and output delimiter to tab, instead of the default space. Change” to your delimiter if its not tab. NR==1{} says to execute the actions only on the first line TO DO : Pipe these commands together Sequence file Replace semi-colon with tabs for i in *.trainning.fasta ; do sed -i &#39;s/;/\\t/g&#39; $i ; done for i in *.RDP_trainID.txt ; do python2 lineage2taxTrain.py $i &gt; ${i%%.*}.ready4train_tax.txt ; done for i in *.trainning.fasta ; do python2 addFullLineage.py ${i%%.*}.RDP_trainID.txt $i &gt; ${i%%.*}.ready4train_seqs.fasta ; done Train the set for i in *.ready4train_tax.txt ; do java -Xmx10g -jar ~/rdp_classifier_2.14/dist/classifier.jar train -o ${i%%.*}_training_files -s ${i%%.*}.ready4train_seqs.fasta -t $i ; done Output is a directory specified by the parameter -o which should contain the following files : bergeyTrainingTree.xml genus_wordConditionalProbList.txt logWordPrior.txt wordConditionalProbIndexArr.txt Move into this newly created directory and create the file rRNAClassifier.properties with the following text : # Sample ResourceBundle properties file bergeyTree=bergeyTrainingTree.xml probabilityList=genus_wordConditionalProbList.txt probabilityIndex=wordConditionalProbIndexArr.txt wordPrior=logWordPrior.txt classifierVersion=RDP Naive Bayesian rRNA Classifier Version 2.14 Generate predictions for i in *.ready4train_seqs.fasta ; do java -Xmx1g -jar ~/rdp_classifier_2.14/dist/classifier.jar -q ${i%%.*}.subsets.txt.testIDs.txt.test.fasta -t ./${i%%.*}_training_files/rRNAClassifier.properties -o ${i%%.*}.predictions.tsv ; done Generate predictions for VSEARCH for i in *.subsets.txt.testIDs.txt.test.fasta ; do vsearch --usearch_global $i -db ${i%%.*}.subsets.txt.trainingIDs.txt.trainning.fasta --blast6out ${i%%.*}.vsearch_prediction.tsv --top_hits_only --id 0.70; done Output is a tabbed text given by the -blast6out option. Fields are: Col1 : query_name Col2 : target Col3 : id Col4 : alnlen Col5 : mism Col6 : opens Col7 : qlo Col8 : qhi Col9 : tlo Col10 : thi Col11 : evalue Col12 : bits CVI metrics Important definitions : N : number of sequences in the test set S, K : number of sequences in S with known names (names which are present in the training set A) L : number of novel test sequences (= N – K) (sequences in S with names that are not present in A) TP : number of names which are correctly predicted MC : number of misclassification errors OC : number of over-classification errors UC : number under-classification errors The rate for each type of error is defined as the number of errors divided by the number of opportunities to make that error: OCR = OC/L (over-classification rate), UCR = UC/K (under-classification rate) MCR = MC/K (misclassification rate) TPR = TP/K Acc = TP/(K + OC) For each rank the mean values of the metrics over all test/training pairs for all values of the top-hit identity (d) was calculated and is designated by prefix Avg. True-positive rate (AvgTPR) Under-classification errors (AvgUCR) Misclassification rate (AvgMCR) Over-classification rate (AvgOCR) Average L10Acc Average accuracy (AvgAcc) The lowest common rank (LCR) of two sequences is the lowest rank where both have the same taxon name. The most probable lowest common rank (MLR) for a pair of sequences with identity d is defined as the LCR with highest probability. MLRs can be summarized by giving the rank identity threshold (RIT) for each rank r. The rank identity threshold (RIT) for each rank r is defined as the minimum identity for which MLR(d) = r. For example, if MLR(100) = species, MLR(99) = genus, MLR(98) = genus, … MLR(94) = genus and MLR(93) = family, then RIT(species) = 100 and RIT(genus) = 94. The top-hit identity distribution (THID) is the distances from a reference database. Script to evaluate prediction "],["required-files-scripts-and-programs.html", "Required files, scripts and programs Programs Databases Scripts", " Required files, scripts and programs Programs Sekquit Installed via conda Base environment needs to be activated in order to use the tool RDP tool classifier - Github / SourceForge To install the RDP classifier I recommend downloading the package from SourceForge (link) as the Github is no longer maintained. Once the download is complete, unzip the folder and locate the file classifer.jar inside the directory dist Databases COI formatted for Barque Eukaryote CO1 Classifier for RDP V5.1 12S fish Classifier v1.0.1 12S formatted for barque Scripts lineage2taxTrain.py addFullLineage.py "],["code.html", "Code Evaluating representation in databases Workflows Curating databases 0.1 Curating databases for distance matrix Comparing pipelines with available databases Curating databases", " Code Evaluating representation in databases #### ------------------------- Load libraries ------------------------------------------------------#### library(tidyverse) #### ------------------------- Load databases sequence header -------------------------------------------------- #### # Header were extracted using shell command : grep &quot;&gt;&quot; file.fasta | sed -e &quot;s/&gt;//&quot; &gt; headers.txt ## RDP classifier ## rdp12s_raw = read.table(&quot;/home/kvilleneuve/fish_edna/database/header_rdp_12S.txt&quot;, sep = &quot;;&quot;) rdpcoi_raw = read.table(&quot;/home/kvilleneuve/fish_edna/database/header_rdp_coi.txt&quot;, sep = &quot;;&quot;) ## barque ## barque12s_raw = read.table(&quot;/home/kvilleneuve/fish_edna/database/header_barque_12S.txt&quot;, sep = &quot;_&quot;) barquecoi_raw = read.table(&quot;/home/kvilleneuve/fish_edna/database/header_bold_coi_barque.txt&quot;, sep = &quot;_&quot;) #### ------------------------- Load list of Freshwater fishes from Alberta ------------------------- #### alberta_fish = read.csv(&quot;/home/kvilleneuve/fish_edna/database/fishbase_alberta_freswater.csv&quot;, header = TRUE, check.names = FALSE) alberta_fish$Species = gsub(&quot; &quot;, &quot;_&quot;, alberta_fish$Species) # Replace space to underscore ### ------------------------- Parse databases dataframes ----------------------------------------- #### # Because the number of ranks differ between databases used with # the RDP classifier and the barque workflow they are processed separately ## RDP databases list_rdp_db = list(&quot;COI RDP&quot; = rdpcoi_raw, &quot;12S RDP&quot; = rdp12s_raw) list_rdp_taxa = list() i = 0 for (databases in list_rdp_db){ i = i + 1 names(databases) = c(&quot;ID&quot;, &quot;Superkingdom&quot;, &quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;, &quot;Order&quot;, &quot;Family&quot;, &quot;Genus&quot;, &quot;Species&quot;) list_rdp_taxa[[i]] = databases names(list_rdp_taxa)[i] = names(list_rdp_db[i]) } ## barque databases list_barque_db = list(&quot;12S barque&quot; = barque12s_raw, &quot;COI barque&quot; = barquecoi_raw) list_barque_taxa = list() i = 0 for (databases in list_barque_db){ i = i + 1 names(databases) = c(&quot;Family&quot;, &quot;Genus&quot;, &quot;sort_species&quot;) databases$Species = paste(databases$Genus,&quot;_&quot;,databases$sort_species, sep =&quot;&quot;) list_barque_taxa[[i]] = databases names(list_barque_taxa)[i] = names(list_barque_db[i]) } # After parsing each databases we combine them into as single list # then we filter out fishes which are not found in ther list of Alberta freshwater fishes jointlist_all_db = c(list_rdp_taxa, list_barque_taxa) list_filtered_db = list() i = 0 for (databses in jointlist_all_db){ i = i + 1 filtered_db = databses %&gt;% filter(Species %in% unique(alberta_fish$Species)) filtered_db_df = as.data.frame(unique(filtered_db$Species)) names(filtered_db_df) = &quot;Species&quot; filtered_db_df[names(jointlist_all_db[i])] = &quot;Yes&quot; list_filtered_db[[i]] = filtered_db_df } filtered_df = Reduce(function(...) merge(..., all=T), list_filtered_db) ### ------------------------- Combine as dataframe as save output ----------------------------------------- #### final_df = merge(alberta_fish, filtered_df, by = &quot;Species&quot;, all = TRUE) final_df$Species = gsub(&quot;_&quot;, &quot; &quot;, final_df$Species) # Reordering columns final_df = final_df[,c(2,3,1,4,5,6,7,8,9,10)] write.csv(final_df, &quot;/home/kvilleneuve/fish_edna/results/represensation_albertafish_databases.csv&quot;, quote = FALSE) Workflows DADA2 12S COI 839 ASV | from these 839 ASVs, 20 (2.38 %) are species with a bootstrap level above 70 % | from these 20 species, 8 (40 %) are from class actinopteri | from these 8 actinopteri, 7 (87.5 %) are species of freshwater fishes found in Alberta. Barque Curating databases Using seqkit tool to filter databases in order to keep only sequences of freshwater fishes found in Alberta seqkit grep -i -f ids.txt test.fa seqkit grep -nrf ID_barque_12S.txt barque_12S.fasta -o alberta_barque_12S.fasta seqkit grep -nrf ID_barque_COI.txt bold_coi_for_barque_2023-09-12.fasta -o alberta_barque_COI.fasta For RDP sed -i &#39;s/ /;/g&#39; RDP_COI.fasta sed -i &#39;s/ /;/g&#39; ID_RDP_COI.txt seqkit grep -nrf ID_RDP_COI.txt RDP_COI.fasta -o alberta_RDP_COI.fasta sed -i &#39;s/\\t/;/g&#39; 12S_rdp.fasta sed -i &#39;s/\\t/;/g&#39; ID_RDP_12S.txt seqkit grep -nrf ID_RDP_12S.txt 12S_rdp.fasta -o alberta_RDP_12S.fasta 0.1 Curating databases for distance matrix 0.1.1 Barque Filtering out sequence which do not belong to phylum Chordata, (2) adding unique ID to every sequence header (required to randomly sample 10 sequences per species), and (3) extracting sequences header. seqkit grep -r -n -p &#39;.*chordata_*&#39; barque_coi.fasta -o chord_barque_coi.fasta awk &#39;/^&gt;/{sub(&quot;&gt;&quot;, &quot;&gt;&quot;++i&quot;_&quot;)}1&#39; chord_barque_coi.fasta &gt; chord_barque_coi_ID.fasta grep &quot;&gt;&quot; chord_barque_coi_ID.fasta | sed -e &quot;s/&gt;//&quot; &gt; barque_coi_ID_header.txt The file with the sequence headers can then be passed to R to generate a list of which headers to keep. As per methodology by Edgar (2018), because the databases have a highly uneven numbers of sequences per genus, a subset is create by imposing a maximum of 10 sequences per genus. Headers are sampled at random to meet this constraint. #### ------------------------- Load libraries ------------------------------------------------------------------#### library(dplyr) #### ------------------------- Load databases sequence header -------------------------------------------------- #### barquecoi = read.table(&quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/barque_coi_ID_header.txt&quot;, sep = &quot;_&quot;) rdpcoi = read.table(&quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/rdp_coi_header.txt&quot;,sep = &quot;;&quot;) #### ------------------------- Format dataframes for merging --------------------------------------------------- #### # The sequence headers of the COI database used by barque has the following format : Phylum_Genus_Species # Therefore, to keep only sequence belonging to class Actinopteri # I am merging the header from the barque COI database with headers from the RDP COI database to get complete # taxonomic rank barquecoi$Species = paste(barquecoi$V2, barquecoi$V3, sep = &quot;_&quot;) # Create column with specie name barquecoi_unique = barquecoi[!duplicated(barquecoi), ] # Keeping only unique value # Add column names to RDP database names(rdpcoi) = c(&quot;ID&quot;, &quot;Cellular_organism&quot;,&quot;Superkingdom&quot;, &quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;, &quot;Order&quot;, &quot;Family&quot;, &quot;Genus&quot;, &quot;Species&quot;) rdpcoi_noID = subset(rdpcoi, select = -c(ID)) # Remove column ID (they are unique values) rdpcoi_unique = rdpcoi_noID[!duplicated(rdpcoi_noID), ] # Keeping only unique value #### ------------------------- Merge dataframes -------------------------------------------------------------- #### comb_df = merge(barquecoi_unique, rdpcoi_unique, by = &quot;Species&quot;, all.y = FALSE ) #### ------------------------- Random sampling --------------------------------------------------------------- #### random_sampling_list = list() i = 0 for (genus in unique(comb_df$Genus.x)){ i = i + 1 sub_df = subset(comb_df, Genus.x == genus) if (nrow(sub_df) &gt; 10) { rand_sub_df = sub_df[sample(nrow(sub_df), size=10), ] } else { rand_sub_df = sub_df } random_sampling_list[[i]] = rand_sub_df } df = do.call(&quot;rbind&quot;, random_sampling_list) #### ------------------------- Generate list of headers to keep ----------------------------------------------- #### df$barque_coi_header = paste(df$ID, df$Phylum.x, df$Species, sep = &quot;_&quot;) write.table(df$barque_coi_header, &quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/barque_coi_headers_to_keep.txt&quot;,col.names = FALSE, row.names = FALSE, quote = FALSE) Pass the file generated by the R script to seqkit to create a subset database. seqkit grep -nrf barque_coi_headers_to_keep.txt chord_barque_coi.fasta -o curated_barque_coi.fasta 0.1.2 RDP Filter out sequence which do not belong to class Actinopteri. seqkit grep -r -n -p &#39;.*Actinopteri*&#39; rdp_coi.fasta -o chord_rdp_coi.fasta #### ------------------------- Load libraries ------------------------------------------------------------------#### library(dplyr) library(tidyr) #### ------------------------- Load databases sequence header -------------------------------------------------- #### rdpcoi = read.table(&quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/rdp_coi_header.txt&quot;,sep = &quot;;&quot;) #### ------------------------- Format dataframes -------------------------------------------------------------- #### names(rdpcoi) = c(&quot;ID&quot;, &quot;Cellular_organism&quot;,&quot;Superkingdom&quot;, &quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;, &quot;Order&quot;, &quot;Family&quot;, &quot;Genus&quot;, &quot;Species&quot;) rdp_coi_actino = subset(rdpcoi, Class == &quot;Actinopteri&quot;) #### ------------------------- Random sampling --------------------------------------------------------------- #### random_sampling_list = list() i = 0 for (genus in unique(rdp_coi_actino$Genus)){ i = i + 1 sub_df = subset(rdp_coi_actino, Genus == genus) if (nrow(sub_df) &gt; 10) { rand_sub_df = sub_df[sample(nrow(sub_df), size=10), ] } else { rand_sub_df = sub_df } random_sampling_list[[i]] = rand_sub_df } #### ------------------------- Generate list of headers to keep ----------------------------------------------- #### df = do.call(&quot;rbind&quot;, random_sampling_list) df = df %&gt;% unite(&quot;sequence_header&quot;, ID:Species, remove = FALSE, sep = &quot;;&quot;) write.table(df$sequence_header, &quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/rdp_coi_headers_to_keep.txt&quot;, col.names = FALSE, row.names = FALSE, quote = FALSE) Pass the file generated by the R script to seqkit to create a subset database. seqkit grep -nrf rdp_coi_headers_to_keep.txt chord_rdp_coi.fasta -o curated_rdp_coi.fasta Comparing pipelines with available databases DADA2 12S COI 839 ASV | from these 839 ASVs, 20 (2.38 %) are species with a bootstrap level above 70 % | from these 20 species, 8 (40 %) are from class actinopteri | from these 8 actinopteri, 7 (87.5 %) are species of freshwater fishes found in Alberta. Barque Curating databases https://forum.qiime2.org/t/building-a-coi-database-from-bold-references/16129 "],["supplementary-data.html", "Supplementary data Installing the RDP classifier", " Supplementary data Installing the RDP classifier The RDP Classifier is a naive Bayesian classifier that provides rapid and accurate taxonomic placement of rRNA gene sequences. Download the RDPTools is a collection fo command-line tools written by the RDP staff at Michigan State University. More information and tutorials on how to install, use and retrain RDP Clasifier can be found on at https://github.com/rdpstaff/classifier and John Quensen’s blog (https://john-quensen.com/). Unfortunately, the The RDP website is no longer available and therefore the packages must be installed using the Docker verison of RDPTools. instruction where obtained from Dr. J.Quensen’s Website. Installing Docker Engine The following installation instruction were obtained directly from the Docker documentation page # --------------------- Run the following command to uninstall all conflicting packages ------------------------ for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done # --------------------- Add Docker&#39;s official GPG key ------------------------ sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # ------------------------ Add the repository to Apt sources ------------------------ echo \\ &quot;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release &amp;&amp; echo &quot;$VERSION_CODENAME&quot;) stable&quot; | \\ sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null sudo apt-get update # ------------------------ Install the Docker packages ------------------------ sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin # ------------------------ Confirm if installation was successful ------------------------ sudo docker run hello-world Creating the container To display all locally available docker images : sudo docker images sudo docker pull rdpstaff/rdp_tools Containers must be map to a directory outside of the container in order to make results accessible to other programs. In this case I chose the directory /home/kvilleneuve/rdp_tools/ sudo docker run --name rdp_tools -it -v /home/kvilleneuve/rdp_tools/ rdpstaff/rdp_tools After entering the above command the prompt terminal should be similar to this : RDPuser@1041715dcc7f:~$ Installing auxiliary programs and files Change to root user. Password (RDPuser) sudo su - # Download script for downloading packages wget https://github.com/jfq3/RDPTools-Docker/raw/main/downloads/download_tools_jq.py # NOTE !!! The script contains a few typo, notable a missing parenthesis and open tick at line 58 !!!! # Make script executable chmod 750 download_tools_jq.py # Execute python3 download_tools_jq.py Log out, exit the container and close the terminal : exit exit exit To launch docker sudo docker start rdp_tools sudo docker attach rdp_tools "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
