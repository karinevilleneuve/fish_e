[["index.html", "Fish eDNA Introduction Objectives Dataset Alberta fishes Methods and their associated available databases Representation of Alberta freshwater fishes in databases", " Fish eDNA Introduction Objectives Dataset The dataset consists of 18 water samples obtained from rivers and lakes near oil-sands extraction sites in the province of Alberta, Canada. Water samples were filtered and DNA collected on the filters was extracted and sequenced. For each sample, two different marker genes were used to amplify and sequence distinct DNA regions : the 12S ribosomal RNA gene and the mitochondrial cytochrome oxidase subunit (COI). Alberta fishes Fishbase was used to generate the following list of Alberta freshwater fishes. Methods and their associated available databases Within the framework of this project, the following methods and their associated databases were evaluated in regards to their capacities of correctly identifying sequences belonging to freshwater fishes from Alberta. Links [1] Barcode of life (BOLD) [2] barque workflow [3] COI formatted for Barque [4] DADA2 workflow [5] Eukaryote CO1 Classifier [6] Mitochondrial Genome Database of Fish (MitoFish) [7] National Center for Biotechnology Information (NCBI) [8] Ribosomal Database Project (RDP) classifier [9] VSEARCH [10] 12S fish Classifier v1.0.1 [11] 12S formatted for barque Representation of Alberta freshwater fishes in databases Each database was queried to evaluate the presence of reference sequences for Alberta freshwater fishes. Code used to queried the databases can be found [here][Evaluating representation in databases]. The absence of the following fishes from any databases can be attributed to different names being used. The presence of these other names in the databases was therefore also evaluated. "],["required-files-scripts-and-programs.html", "Required files, scripts and programs Programs Databases Scripts", " Required files, scripts and programs Programs seqkit v2.1 (website) Installed via Bioconda (link) Base environment needs to be activated in order to use the tool RDP tool classifier v2.11 (Github / SourceForge) To install the RDP classifier I recommend downloading the package from SourceForge as the Github is no longer maintained. Once the download is complete, unzip the folder and locate the file classifer.jar inside the directory dist usearch v11.0.667 (website) To install : (1) Download binaries (2) Unzip and rename the file to usearch (3) Make file executable chmod +x usearch. Place file in an easily accessible directory (for example ~/usearch) vsearch v2.15.2 (Github) Installed via Bioconda (link) Base environment needs to be activated in order to use the tool Databases The following databases can be downloaded be following the hyperlinks. COI formatted for Barque Eukaryote CO1 Classifier for RDP V5.1 12S fish Classifier v1.0.1 12S formatted for barque Scripts In order to train the RDP classifier on any database the following scripts are required and can be found here. lineage2taxTrain.py addFullLineage.py "],["taxonomy-cross-validation-by-identity-taxxi.html", "Taxonomy cross validation by identity (TAXXI) Description Methods Implementing CVI with our data Generate benchmark datasets Generate predictions CVI metrics", " Taxonomy cross validation by identity (TAXXI) Link to article and TAXXI website. Description Edgar (2018) developed a Cross-Validation by Identity (CVI) framework that tests 16 unique classifiers and 3 parameter settings of the SINTAX, RDP, and Non-Bayensien-Classifier classifiers to assign taxonomy to specially modified test and training data sets (Warcup ITS, 16S full length, 16S V4 and 16S V3-5). These data sets were designed by Edgar (2018) to 1) have even representation across genera, and 2) test classifier effectiveness across different loci of the same gene (16S). Each classifier is then assessed for the true positive rate (TPR), the over-classification rate (OCR), the under classification rate (UCR), the misclassification rate (MCR), and the accuracy (ACC). The true positive rate (TPR) indicates how frequently the correct taxonomy was assigned out of the total number of opportunities for correct classification. The over-classification rate (OCR) indicates how frequently too many ranks are predicted for query sequences out of the total opportunities to make an over classification error. The under-classification rate (UCR) indicates how frequently too few ranks are predicted for query sequences out of the total number of opportunities to make this error. The misclassification rate (MCR) indicates how frequently a sequence matching a query is available in the database but is not predicted for that query out to the number of opportunities to make this error. The accuracy (ACC) indicates the number of correct taxonomic calls out of the number of opportunities to determine correct taxonomy. Methods A reference with known taxonomies is split into test and training sets such that for all test sequences, the most similar training sequence has a given identity (d). This is repeated for different identities, enabling assessment of prediction accuracy at varying distances from the reference. - R is the reference dataset divided into four disjoint subsets S, T, W and Z. - S is the test set. - A is the training set formed by the union of T and W. - T is the set of top hits for sequences in S, which are constrained to have identities in the range d ± σ (where σ specifies the maximum allowed deviation from the desired identity (d)). - W contains reference sequences with identity &lt; d; these are retained to create the largest possible training set. - Z contains sequences which cannot be assigned to S, T or W without violating the identity constraint. Implementing CVI with our data TAXXI framework was used to compare the performance of the RDP and vsearch classifiers with the following databases : barque COI 3 RDP COI V5.1.0 5 Considering the size of the original COI databases, the memory limit of 32-bit process was exceeded, and therefore the 64-bit build was required. To overcome this issue both databases were pre-curated and sub-sampled. Workflow used for this can be found here. Because the 12S databases did not include multiple sequences per species the TAXXI framework could not be used to evaluate these databases. Generate benchmark datasets The distmx_split_identity command from USEARCH divides sequences into subsets such that the top-hit identity is a given value. This is used to create test-training pairs for cross-validation by identity. Input is a distance matrix created by the calc_distmx command. As per the methods specified in Edgar (2018) maximum allowed deviation (σ) from d used : σ = 1% for d = 90% and σ = 0.5% for d = 99, 97 and 95%. The following code was adapted from Donhauser et al. (2024) and validated with available documentation from USEARCH. 1. Format databases Remove tab spaces in sequence name (for RDP databases specifically) for i in *.fasta ; do sed -i &#39;s/\\t/;/g&#39; $i ; done All the sequences from the COI database used with RDP were lowercase. The following code was used to make them uppercase seqkit seq mytrainseq.fasta --upper-case -w 0 &gt; rdp_coi.fasta 2. Generate trainning and test sets Create a distance matrix using function -calc_distmx for i in *.fasta ; do ~/usearch -calc_distmx $i -maxdist 0.2 -termdist 0.3 -tabbedout ${i%%.*}_distmax.txt; done To create training and test sets at different identity thresholds for i in *.fasta ; do ~/usearch -distmx_split_identity ${i%%.*}_distmax.txt -mindist 0.025 -maxdist 0.035 -tabbedout ${i%%.*}.97.subsets.txt ~/usearch -distmx_split_identity ${i%%.*}_distmax.txt -mindist 0.045 -maxdist 0.055 -tabbedout ${i%%.*}.95.subsets.txt ~/usearch -distmx_split_identity ${i%%.*}_distmax.txt -mindist 0.095 -maxdist 0.105 -tabbedout ${i%%.*}.90.subsets.txt done Output is a tabbed text given by the -tabbedout option. Fields are: Col1 - Subset name : there are four subsets with names 1, 2, 1x and 2x. Col2 - Label1 : label1 is the label of a sequence in the subset given by Col1. Col3 - Label2 : label2 is the top hit in the other subset (1 or 2) Col4 - Dist : distance between Label1 and Label2. Subsets 1 and 2 have top hits to each other in the specified range. Subset 1x has lower identities with subset 2, and can therefore be added to the training set if subset 2 is the query set. Similarly, subset 2x has lower identities with subset 1 and can be added to the training set if subset 1 is the query Get sequence ID for training and test set for i in *.subsets.txt ; do awk &#39;$1==1 || $1==&quot;1x&quot; {print $2}&#39; $i &gt; $i.trainingIDs.txt; awk &#39;$1==2 {print $2}&#39; $i &gt; $i.testIDs.txt; done Create fasta file with test and training set at each identity, use subset 1 as training and subset 2 as test. If sekqit is installed through conda activate base environment first. for i in *.trainingIDs.txt ; do seqkit grep -n -f $i ${i%%.*}.fasta &gt; $i.trainning.fasta ; done for i in *.testIDs.txt ; do seqkit grep -n -f $i ${i%%.*}.fasta &gt; $i.test.fasta ; done 3. Rename files for file in *subsets* ; do mv $file ${file//.9/_9} ; done Generate predictions RDP Classifier Following steps were adapted from John Quensen’s tutorial (link) and available scripts associated with publication by Miranda (2020). Required files To train the newly generated training set for each identity level you need to generate a sequence file and a taxonomy file, each with special formatting requirements : Sequence file The sequence file must be in fasta format and contain a unique identifier without any white space. The accession number makes a good identifier. Anything after the first white space is ignored. The following are acceptable: &gt;DQ248313 ACGATTTTGACCCTTCGGGGTCGATCTCCAACCCTTCGGGGTCGATCGATTTTGACCCT &gt;JF735302 k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Hypocreales;f__Nectriaceae;g__Dactylonectria;s__Dactylonectria_anthuriicol CCGAGTTTTCAACTCGACCCTTCGGGGTCGTCGATCTCCAACCCGATCGATTTTGAACC Taxonomy file The taxonomy file is a tab-delimited text file beginning with a header giving the Sequence ID and names of ranks to be included. There are two requirements for this file: There must be an entry for every rank in every line. Hyphen placeholders are allowed but are not recommended. “Convergent evolution” is not allowed. For example, the same genus cannot appear in two different families. Options for missing ranks : If a rank does not exist, you can fill in the missing entries with hyphens but it is not recommended as it can lead to a “ragged” classification that cannot be properly sorted by rank. Another option is to fill in the empty spaces with made-up but meaningful ranks as in the table below. The prefixes indicate the highest rank available. The absence of hyphen placeholders means that classification will not be ragged but include all ranks. Thus it will be possible to select, sort, and merge ranks when analyzing your data later. Example format : Seq_ID Kingdom Phylum Class Order Family Genus Species MG190602 Fungi Ascomycota Sordariomycetes Hypocreales o_Hypocreales o_f_Hypocreales MF120484 Fungi Ascomycota Sordariomycetes Hypocreales Nectriaceae Fusarium Scripts lineage2taxTrain.py addFullLineage.py Generate required files Taxonomy file Extract sequence header using grep for i in *.trainning.fasta ; do grep -e &quot;&gt;&quot; $i &gt; ${i%%.*}_trainheader.txt ; done For RDP databases which have the following sequence header format we can simply replace semi-colon with tabs &gt;KJ592636 cellularOrganisms;Eukaryota;undef_Eukaryota;Rhodophyta;Florideophyceae;Hapalidiales;Mesophyllumaceae;Mesophyllum;Mesophyllum_lichenoides For COI databases which have the following format, and Phylum being exclusively chordata we will replace the string _chordata_ with a tab &gt;11_chordata_Abalistes_stellaris for i in *.trainingIDs.txt ; do sed &#39;s/;/\\t/g&#39; $i &gt; ${i%%.*}.RDP_trainID.txt ; done for i in *.trainingIDs.txt ; do sed &#39;s/_chordata_/\\t/g&#39; $i &gt; ${i%%.*}.RDP_trainID.txt ; done Remove “&gt;” for i in *.RDP_trainID.txt ; do sed -i &#39;s/&gt;//g&#39; $i ; done Add header (rank followed by column number i.e. rank_1, rank_2, etc.) for i in *.RDP_trainID.txt ; do awk -i inplace &#39;BEGIN {OFS=FS=&quot;\\t&quot;} NR==1{for (i=1;i&lt;=NF;i++) printf &quot;%s%s&quot;, &quot;rank_&quot;i, i==NF?ORS:OFS}1&#39; $i ; done Details : BEGIN {OFS=FS=“} sets the input and output delimiter to tab, instead of the default space. Change” to your delimiter if its not tab. NR==1{} says to execute the actions only on the first line Sequence file For RDP databases which have the following sequence header format we can simply replace semi-colon with tabs For COI databases which have the following format, and Phylum being exclusively chordata we will replace the string _chordata_ with tabs for i in *.trainning.fasta ; do sed -i &#39;s/;/\\t/g&#39; $i ; done for i in *.trainning.fasta ; do sed -i &#39;s/_chordata_/\\t/g&#39; $i ; done for i in *.RDP_trainID.txt ; do python2 lineage2taxTrain.py $i &gt; ${i%%.*}.ready4train_tax.txt ; done for i in *.trainning.fasta ; do python2 addFullLineage.py ${i%%.*}.RDP_trainID.txt $i &gt; ${i%%.*}.ready4train_seqs.fasta ; done Train the set for i in *.ready4train_tax.txt ; do java -Xmx10g -jar ~/rdp_classifier_2.14/dist/classifier.jar train -o ${i%%.*}_training_files -s ${i%%.*}.ready4train_seqs.fasta -t $i ; done Output is a directory specified by the parameter -o which should contain the following files : bergeyTrainingTree.xml genus_wordConditionalProbList.txt logWordPrior.txt wordConditionalProbIndexArr.txt Move into this newly created directory and create the file rRNAClassifier.properties with the following text : # Sample ResourceBundle properties file bergeyTree=bergeyTrainingTree.xml probabilityList=genus_wordConditionalProbList.txt probabilityIndex=wordConditionalProbIndexArr.txt wordPrior=logWordPrior.txt classifierVersion=RDP Naive Bayesian rRNA Classifier Version 2.14 Generate predictions for i in *.ready4train_seqs.fasta ; do java -Xmx1g -jar ~/rdp_classifier_2.14/dist/classifier.jar -q ${i%%.*}.subsets.txt.testIDs.txt.test.fasta -t ./${i%%.*}_training_files/rRNAClassifier.properties -o ${i%%.*}.RDP_predictions.tsv ; done VSEARCH Using option usearch_global (option used by barque) : for i in *.subsets.txt.testIDs.txt.test.fasta ; do vsearch --usearch_global $i -db ${i%%.*}.subsets.txt.trainingIDs.txt.trainning.fasta --blast6out ${i%%.*}.vsearch_predictions.tsv --top_hits_only --notrunclabels --id 0.70; done Output is a tabbed text given by the -blast6out option. Fields are: Col1 : query_name Col2 : target - database sequence label Col3 : id - percentage of identity (real value ranging from 0.0 to 100.0). The per- centage identity is defined as 100 * (matching columns) / (alignment length - terminal gaps). See fields id0 to id4 for other definitions. Col4 : alnlen Col5 : mism Col6 : opens Col7 : qlo Col8 : qhi Col9 : tlo Col10 : thi Col11 : evalue Col12 : bits Using SINTAX CVI metrics Important definitions : N : number of sequences in the test set S, K : number of sequences in S with known names (names which are present in the training set A) L : number of novel test sequences (= N – K) (sequences in S with names that are not present in A) TP : number of names which are correctly predicted MC : number of misclassification errors OC : number of over-classification errors UC : number under-classification errors The rate for each type of error is defined as the number of errors divided by the number of opportunities to make that error: OCR = OC/L (over-classification rate), UCR = UC/K (under-classification rate) MCR = MC/K (misclassification rate) TPR = TP/K Acc = TP/(K + OC) For each rank the mean values of the metrics over all test/training pairs for all values of the top-hit identity (d) was calculated and is designated by prefix Avg. True-positive rate (AvgTPR) Under-classification errors (AvgUCR) Misclassification rate (AvgMCR) Over-classification rate (AvgOCR) Average L10Acc Average accuracy (AvgAcc) The lowest common rank (LCR) of two sequences is the lowest rank where both have the same taxon name. The most probable lowest common rank (MLR) for a pair of sequences with identity d is defined as the LCR with highest probability. MLRs can be summarized by giving the rank identity threshold (RIT) for each rank r. The rank identity threshold (RIT) for each rank r is defined as the minimum identity for which MLR(d) = r. For example, if MLR(100) = species, MLR(99) = genus, MLR(98) = genus, … MLR(94) = genus and MLR(93) = family, then RIT(species) = 100 and RIT(genus) = 94. The top-hit identity distribution (THID) is the distances from a reference database. Script to evaluate prediction Required files for each identity threshold : Training set header ({databasename_IDthreshold}_trainheader.txt) Test set header ({databasename_IDthreshold}.subsets.txt.testIDs.txt) Prediction results ({databasename_IDthreshold_classifier}_predictions.tsv) Move required files in a different directory "],["supplementary-workflow.html", "Supplementary workflow Curating databases for distance matrix Worklow used Curating databases for Alberta freshwater fish", " Supplementary workflow Curating databases for distance matrix Required files : barque COI 3 (barque_coi.fasta) RDP COI V5.1.0 5 (rdp_coi.fasta) RDP Filter out sequence which do not belong to class Actinopteri seqkit grep -r -n -p &#39;.*Actinopteri*&#39; rdp_coi.fasta -o chord_rdp_coi.fasta Extract sequences header and print into txt file grep &quot;&gt;&quot; chord_rdp_coi.fasta | sed -e &quot;s/&gt;//&quot; &gt; rdp_coi_header.txt The file with the sequence headers can then be passed to R to generate a list of which headers to keep. As per methodology by Edgar (2018), because the databases have a highly uneven numbers of sequences per genus, a subset is create by imposing a maximum of 10 sequences per genus. Headers are sampled at random to meet this constraint. #### ------------------------- Load libraries ------------------------------------------------------------------#### library(dplyr) library(tidyr) #### ------------------------- Load databases sequence header -------------------------------------------------- #### rdpcoi = read.table(&quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/rdp_coi_header.txt&quot;,sep = &quot;;&quot;) #### ------------------------- Format dataframes -------------------------------------------------------------- #### names(rdpcoi) = c(&quot;ID&quot;, &quot;Cellular_organism&quot;,&quot;Superkingdom&quot;, &quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;, &quot;Order&quot;, &quot;Family&quot;, &quot;Genus&quot;, &quot;Species&quot;) rdp_coi_actino = subset(rdpcoi, Class == &quot;Actinopteri&quot;) #### ------------------------- Random sampling --------------------------------------------------------------- #### random_sampling_list = list() i = 0 for (genus in unique(rdp_coi_actino$Genus)){ i = i + 1 sub_df = subset(rdp_coi_actino, Genus == genus) if (nrow(sub_df) &gt; 10) { rand_sub_df = sub_df[sample(nrow(sub_df), size=10), ] } else { rand_sub_df = sub_df } random_sampling_list[[i]] = rand_sub_df } #### ------------------------- Generate list of headers to keep ----------------------------------------------- #### df = do.call(&quot;rbind&quot;, random_sampling_list) df = df %&gt;% unite(&quot;sequence_header&quot;, ID:Species, remove = FALSE, sep = &quot;;&quot;) write.table(df$sequence_header, &quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/rdp_coi_headers_to_keep.txt&quot;, col.names = FALSE, row.names = FALSE, quote = FALSE) Pass the file generated by the R script to seqkit to create a subset database seqkit grep -nrf rdp_coi_headers_to_keep.txt chord_rdp_coi.fasta -o curated_rdp_coi.fasta Barque Remove sequence of taxa which do not belong to phylum Chordata seqkit grep -r -n -p &#39;.*chordata_*&#39; barque_coi.fasta -o chord_barque_coi.fasta Add unique ID to every sequence header, this is required to randomly sample 10 sequences per genus awk &#39;/^&gt;/{sub(&quot;&gt;&quot;, &quot;&gt;&quot;++i&quot;_&quot;)}1&#39; chord_barque_coi.fasta &gt; chord_barque_coi_ID.fasta Extract sequences header and print into txt file grep &quot;&gt;&quot; chord_barque_coi_ID.fasta | sed -e &quot;s/&gt;//&quot; &gt; barque_coi_ID_header.txt grep &quot;&gt;&quot; rdp_coi.fasta | sed -e &quot;s/&gt;//&quot; &gt; rdp_coi_header.txt The file with the sequence headers can then be passed to R to generate a list of which headers to keep. As per methodology by Edgar (2018), because the databases have a highly uneven numbers of sequences per genus, a subset is create by imposing a maximum of 10 sequences per genus. Headers are sampled at random to meet this constraint. #### ------------------------- Load libraries ------------------------------------------------------------------#### library(dplyr) #### ------------------------- Load databases sequence header -------------------------------------------------- #### barquecoi = read.table(&quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/barque_coi_ID_header.txt&quot;, sep = &quot;_&quot;) rdpcoi = read.table(&quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/rdp_coi_header.txt&quot;,sep = &quot;;&quot;) #### ------------------------- Format dataframes for merging --------------------------------------------------- #### # The sequence headers of the COI database used by barque has the following format : Phylum_Genus_Species # Therefore, to keep only sequence belonging to class Actinopteri # I am merging the header from the barque COI database with headers from the RDP COI database to get complete # taxonomic rank barquecoi$Species = paste(barquecoi$V2, barquecoi$V3, sep = &quot;_&quot;) # Create column with specie name barquecoi_unique = barquecoi[!duplicated(barquecoi), ] # Keeping only unique value # Add column names to RDP database names(rdpcoi) = c(&quot;ID&quot;, &quot;Cellular_organism&quot;,&quot;Superkingdom&quot;, &quot;Kingdom&quot;, &quot;Phylum&quot;, &quot;Class&quot;, &quot;Order&quot;, &quot;Family&quot;, &quot;Genus&quot;, &quot;Species&quot;) rdpcoi_noID = subset(rdpcoi, select = -c(ID)) # Remove column ID (they are unique values) rdpcoi_unique = rdpcoi_noID[!duplicated(rdpcoi_noID), ] # Keeping only unique value #### ------------------------- Merge dataframes -------------------------------------------------------------- #### comb_df = merge(barquecoi_unique, rdpcoi_unique, by = &quot;Species&quot;, all.y = FALSE ) #### ------------------------- Random sampling --------------------------------------------------------------- #### random_sampling_list = list() i = 0 for (genus in unique(comb_df$Genus.x)){ i = i + 1 sub_df = subset(comb_df, Genus.x == genus) if (nrow(sub_df) &gt; 10) { rand_sub_df = sub_df[sample(nrow(sub_df), size=10), ] } else { rand_sub_df = sub_df } random_sampling_list[[i]] = rand_sub_df } df = do.call(&quot;rbind&quot;, random_sampling_list) #### ------------------------- Generate list of headers to keep ----------------------------------------------- #### df$barque_coi_header = paste(df$ID, df$Phylum.x, df$Species, sep = &quot;_&quot;) write.table(df$barque_coi_header, &quot;/home/kvilleneuve/fish_edna/database/taxxi_cvi/COI_DB/barque_coi_headers_to_keep.txt&quot;,col.names = FALSE, row.names = FALSE, quote = FALSE) Pass the file generated by the R script to seqkit to create a subset database. seqkit grep -nrf barque_coi_headers_to_keep.txt chord_barque_coi.fasta -o curated_barque_coi.fasta Worklow used DADA2 12S COI 839 ASV | from these 839 ASVs, 20 (2.38 %) are species with a bootstrap level above 70 % | from these 20 species, 8 (40 %) are from class actinopteri | from these 8 actinopteri, 7 (87.5 %) are species of freshwater fishes found in Alberta. Barque NB. Barque will only work on GNU Linux or OSX Install dependencies bash 4+ python 3.5+ (you can use miniconda3 to install python) python distutils package R 3+ (ubuntu/mint: sudo apt-get install r-base-core) java (ubuntu/mint: sudo apt-get install default-jre) gnu parallel For mac OSX brew install parallel flash (read merger) v1.2.11+ export PATH=/home/user/FLASH-1.2.11:$PATH vsearch v2.14.2+ (Barque will not work with older versions of vsearch) Depending on how vsearch is installed either export PATH to location of vsearch or activate base conda environnement Download a copy of the Barque repository git clone https://github.com/enormandeau/barque Edit 02_info/primers.csv to provide information describing your primers and database to use Get or prepare the database(s) (see Formatting database section below) and deposit the fasta.gz file in the 03_databases folder and give it a name that matches the information of the 02_info/primers.csv file. Link to BOLD database Modify the parameters in 02_info/barque_config.sh for your run for COI marker : COI_kv,CGTATTTGGYGCYTGRGCCGGRATAGT,CARAARCTYATRTTRTTYATTCG,100,450,bold,0.97,0.9,0.85 for 12S marker 12s200pb,GTCGGTAAAACTCGTGCCAGC,CATAGTGGGGTATCTAATCCCAGTTTG,150,350,12S,0.98,0.9,0.85 Launch Barque Move into the Github directory and launch barque ./barque 02_info/barque_config.sh Once the pipeline has finished running, all result files are found in the 12_results folder. After a run, it is recommended to make a copy of this folder with some additional info cp -r 12_results 12_results_PROJECT_NAME_DATE_SOME_ADDITIONAL_INFO Curating databases for Alberta freshwater fish Using seqkit tool to filter databases in order to keep only sequences of freshwater fishes found in Alberta seqkit grep -i -f ids.txt test.fa seqkit grep -nrf ID_barque_12S.txt barque_12S.fasta -o alberta_barque_12S.fasta seqkit grep -nrf ID_barque_COI.txt bold_coi_for_barque_2023-09-12.fasta -o alberta_barque_COI.fasta For RDP sed -i &#39;s/ /;/g&#39; RDP_COI.fasta sed -i &#39;s/ /;/g&#39; ID_RDP_COI.txt seqkit grep -nrf ID_RDP_COI.txt RDP_COI.fasta -o alberta_RDP_COI.fasta sed -i &#39;s/\\t/;/g&#39; 12S_rdp.fasta sed -i &#39;s/\\t/;/g&#39; ID_RDP_12S.txt seqkit grep -nrf ID_RDP_12S.txt 12S_rdp.fasta -o alberta_RDP_12S.fasta "],["supplementary-data.html", "Supplementary data", " Supplementary data "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
